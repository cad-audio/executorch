diff --git a/backends/cadence/cadence.cmake b/backends/cadence/cadence.cmake
index cb6a2531..0fa55c6a 100644
--- a/backends/cadence/cadence.cmake
+++ b/backends/cadence/cadence.cmake
@@ -44,7 +44,7 @@ set(CMAKE_CXX_COMPILER ${TOOLCHAIN_HOME}/bin/${CROSS_COMPILE_TARGET}-clang++)
 set(CMAKE_C_FLAGS_INIT "-stdlib=libc++ -mtext-section-literals -mlongcalls")
 set(CMAKE_CXX_FLAGS_INIT "-stdlib=libc++ -mtext-section-literals -mlongcalls")
 #workaround for larger compilation time
-SET(CMAKE_CXX_FLAGS_INIT "${CMAKE_CXX_FLAGS_INIT} -fno-strict-aliasing")
+set(CMAKE_CXX_FLAGS_INIT "${CMAKE_CXX_FLAGS_INIT} -fno-strict-aliasing")
 
 set(CMAKE_SYSROOT ${TOOLCHAIN_HOME}/${SYSROOT_TARGET})
 set(CMAKE_LINKER ${TOOLCHAIN_HOME}/bin/xt-ld)
diff --git a/backends/cadence/hifi/kernels/kernels.h b/backends/cadence/hifi/kernels/kernels.h
index 8faf0671..a08144d9 100644
--- a/backends/cadence/hifi/kernels/kernels.h
+++ b/backends/cadence/hifi/kernels/kernels.h
@@ -16,21 +16,24 @@
 #include "xa_nnlib_kernels_api.h"
 
 /* Potential NNLIB function/APIs */
-extern "C" WORD32 xa_nn_elm_add_broadcast_4D_f32xf32_f32(FLOAT32 * __restrict__ p_out,
+extern "C" WORD32 xa_nn_elm_add_broadcast_4D_f32xf32_f32(
+                                FLOAT32 * __restrict__ p_out,
                                 const WORD32 *const p_out_shape,
                                 const FLOAT32 * __restrict__ p_inp1,
                                 const WORD32 *const p_inp1_shape,
                                 const FLOAT32 * __restrict__ p_inp2,
                                 const WORD32 *const p_inp2_shape);
 
-extern "C" WORD32 xa_nn_elm_div_broadcast_4D_f32xf32_f32(FLOAT32 * __restrict__ p_out,
+extern "C" WORD32 xa_nn_elm_div_broadcast_4D_f32xf32_f32(
+                                FLOAT32 * __restrict__ p_out,
                                 const WORD32 *const p_out_shape,
                                 const FLOAT32 * __restrict__ p_inp1,
                                 const WORD32 *const p_inp1_shape,
                                 const FLOAT32 * __restrict__ p_inp2,
                                 const WORD32 *const p_inp2_shape);
 
-extern "C" WORD32 xa_nn_elm_div_mode_f32xf32_f32(FLOAT32 * __restrict__ p_out,
+extern "C" WORD32 xa_nn_elm_div_mode_f32xf32_f32(
+                                FLOAT32 * __restrict__ p_out,
                                 const FLOAT32 * __restrict__ p_inp1,
                                 const FLOAT32 * __restrict__ p_inp2,
                                 WORD32 num_elm,
@@ -45,7 +48,8 @@ extern "C" WORD32 xa_nn_elm_div_mode_broadcast_4D_f32xf32_f32(
                                     const WORD32 *const p_inp2_shape,
                                     WORD32 mode);        
 
-extern "C" WORD32 xa_nn_elm_mul_broadcast_4D_f32xf32_f32(FLOAT32 * __restrict__ p_out,
+extern "C" WORD32 xa_nn_elm_mul_broadcast_4D_f32xf32_f32(
+                                FLOAT32 * __restrict__ p_out,
                                 const WORD32 *const p_out_shape,
                                 const FLOAT32 * __restrict__ p_inp1,
                                 const WORD32 *const p_inp1_shape,
diff --git a/backends/cadence/hifi/operators/op_add.cpp b/backends/cadence/hifi/operators/op_add.cpp
index 883cc74d..56adab71 100644
--- a/backends/cadence/hifi/operators/op_add.cpp
+++ b/backends/cadence/hifi/operators/op_add.cpp
@@ -6,13 +6,13 @@
  * LICENSE file in the root directory of this source tree.
  */
 
+#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 #include <executorch/kernels/portable/cpu/scalar_utils.h>
 #include <executorch/kernels/portable/cpu/util/broadcast_util.h>
 #include <executorch/kernels/portable/cpu/util/functional_util.h>
 #include <executorch/kernels/portable/cpu/util/kernel_ops_util.h>
 #include <executorch/runtime/kernel/kernel_includes.h>
 #include <executorch/runtime/platform/assert.h>
-#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 
 using exec_aten::Scalar;
 using exec_aten::ScalarType;
@@ -23,7 +23,7 @@ using executorch::runtime::KernelRuntimeContext;
 using torch::executor::Error;
 
 namespace impl {
-namespace HiFi { 
+namespace HiFi {
 namespace native {
 
 namespace {
@@ -97,14 +97,15 @@ Tensor& add_out(
 
   ScalarType a_type = a.scalar_type();
   ScalarType b_type = b.scalar_type();
-  ScalarType alpha_type = torch::executor::native::utils::get_scalar_dtype(alpha);
+  ScalarType alpha_type = 
+    torch::executor::native::utils::get_scalar_dtype(alpha);
   ScalarType common_type = promoteTypes(a_type, b_type, /*half_to_float*/ true);
   ScalarType out_type = out.scalar_type();
 
   ET_KERNEL_CHECK(ctx, canCast(common_type, out_type), InvalidArgument, out);
   ET_KERNEL_CHECK(
       ctx, check_alpha_type(alpha_type, common_type), InvalidArgument, out);
-      
+    
   float alpha_val;
   torch::executor::native::utils::extract_scalar(alpha, &alpha_val);
 
@@ -119,30 +120,28 @@ Tensor& add_out(
   const bool broadcast = (a_is_broadcasted || b_is_broadcasted);
   int max_dim = a.dim() > b.dim() ? a.dim() : b.dim();
   max_dim = out.dim() > max_dim ? out.dim() : max_dim;
-  
-  if((out_type != ScalarType::Float) || (alpha_val != 1.0))
+
+  if ((out_type != ScalarType::Float) || (alpha_val != 1.0))
     optimized = 0;
-  
-  if((a_dim == 0) || (b_dim == 0) )
+
+  if ((a_dim == 0) || (b_dim == 0) )
     optimized = 0;
 
-  if((broadcast == 1) && (max_dim > kNnlibMaxDim))
+  if ((broadcast == 1) && (max_dim > kNnlibMaxDim))
     optimized = 0;
 
 
-  if(optimized)
-  {
+  if (optimized) {
       const float* const a_data = a.const_data_ptr<float>();
       const float* const b_data = b.const_data_ptr<float>();
       float* const out_data = out.mutable_data_ptr<float>();
-      if(broadcast == 1)
-      {
+
+      if(broadcast == 1) {
          int out_shape[kNnlibMaxDim];
          int inp1_shape[kNnlibMaxDim];
          int inp2_shape[kNnlibMaxDim];
          
-         for(int i = 0; i < kNnlibMaxDim; i++)
-         {
+         for (int i = 0; i < kNnlibMaxDim; i++) {
             out_shape[i] = 1;
             inp1_shape[i] = 1;
             inp2_shape[i] = 1;
@@ -152,15 +151,15 @@ Tensor& add_out(
          int off_a = kNnlibMaxDim - a.dim();
          int off_b = kNnlibMaxDim - b.dim();
          
-         for(int i = 0; i < out.dim(); i++)
+         for (int i = 0; i < out.dim(); i++)
              out_shape[i+off_o] = out.size(i);
-         for(int i = 0; i < a.dim(); i++)
+         for (int i = 0; i < a.dim(); i++)
              inp1_shape[i+off_a] = a.size(i);
-         for(int i = 0; i < b.dim(); i++)
+         for (int i = 0; i < b.dim(); i++)
              inp2_shape[i+off_b] = b.size(i);
          
-         xa_nn_elm_add_broadcast_4D_f32xf32_f32(out_data, out_shape, a_data, inp1_shape,
-                                                b_data, inp2_shape);
+         xa_nn_elm_add_broadcast_4D_f32xf32_f32(
+           out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape);
       }                      
       else
       {
@@ -193,6 +192,6 @@ Tensor& add_out(
 }
 
 
-} // namespace impl
-} // namespace HiFi
 } // namespace native
+} // namespace HiFi
+} // namespace impl
diff --git a/backends/cadence/hifi/operators/op_div.cpp b/backends/cadence/hifi/operators/op_div.cpp
index 41220e5d..e887e8b5 100644
--- a/backends/cadence/hifi/operators/op_div.cpp
+++ b/backends/cadence/hifi/operators/op_div.cpp
@@ -6,6 +6,7 @@
  * LICENSE file in the root directory of this source tree.
  */
 
+#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 #include <executorch/kernels/portable/cpu/scalar_utils.h>
 #include <executorch/kernels/portable/cpu/util/broadcast_util.h>
 #include <executorch/kernels/portable/cpu/util/functional_util.h>
@@ -13,7 +14,6 @@
 #include <executorch/runtime/kernel/kernel_includes.h>
 #include <executorch/runtime/platform/assert.h>
 #include <cmath> 
-#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 
 using exec_aten::Scalar;
 using exec_aten::ScalarType;
@@ -22,7 +22,7 @@ using executorch::aten::RuntimeContext;
 using torch::executor::Error;
 
 namespace impl {
-namespace HiFi { 
+namespace HiFi {
 namespace native {
 
 namespace {
@@ -74,29 +74,27 @@ div_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
   int max_dim = a.dim() > b.dim() ? a.dim() : b.dim();
   max_dim = out.dim() > max_dim ? out.dim() : max_dim;
   
-  if((a_type != ScalarType::Float) || (b_type != ScalarType::Float))
+  if ((a_type != ScalarType::Float) || (b_type != ScalarType::Float))
     optimized = 0;
   
-  if((a_dim == 0) || (b_dim == 0) )
+  if ((a_dim == 0) || (b_dim == 0) )
     optimized = 0;
 
-  if((broadcast == 1) && (max_dim > kNnlibMaxDim))
+  if ((broadcast == 1) && (max_dim > kNnlibMaxDim))
     optimized = 0;
   
-  if(optimized)
-  {
+  if (optimized) {
     float* a_data = a.mutable_data_ptr<float>();
     float* b_data = b.mutable_data_ptr<float>();
     float* out_data = out.mutable_data_ptr<float>();
     
-    if(broadcast == 1)
-    {
+    if (broadcast == 1) {
       
       int out_shape[kNnlibMaxDim];
       int inp1_shape[kNnlibMaxDim];
       int inp2_shape[kNnlibMaxDim];
       
-      for(int i = 0; i < kNnlibMaxDim; i++)
+      for (int i = 0; i < kNnlibMaxDim; i++)
       {
         out_shape[i] = 1;
         inp1_shape[i] = 1;
@@ -106,34 +104,35 @@ div_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
       int off_o = kNnlibMaxDim - out.dim();
       int off_a = kNnlibMaxDim - a.dim();
       int off_b = kNnlibMaxDim - b.dim();
-      for(int i = 0; i < out.dim(); i++)
+      for (int i = 0; i < out.dim(); i++)
         out_shape[i+off_o] = out.size(i);
-      for(int i = 0; i < a.dim(); i++)
+      for (int i = 0; i < a.dim(); i++)
         inp1_shape[i+off_a] = a.size(i);
-      for(int i = 0; i < b.dim(); i++)
+      for (int i = 0; i < b.dim(); i++)
         inp2_shape[i+off_b] = b.size(i);
       
-      xa_nn_elm_div_broadcast_4D_f32xf32_f32(out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape);
+      xa_nn_elm_div_broadcast_4D_f32xf32_f32(
+        out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape);
     }
     else
     {
-
       xa_nn_elm_div_f32xf32_f32(out_data, a_data, b_data, out.numel());
     }
-    
+
     return out;
   }
-  
+
   ScalarType common_type = get_compute_type(a_type, b_type);
   ScalarType out_type = out.scalar_type();
-  
+
   ET_KERNEL_CHECK(ctx, canCast(common_type, out_type), InvalidArgument, out);
-  
+
   ET_SWITCH_REAL_TYPES_AND(Bool, a_type, ctx, "div.out", CTYPE_A, [&]() {
     ET_SWITCH_REAL_TYPES_AND(Bool, b_type, ctx, "div.out", CTYPE_B, [&]() {
       ET_SWITCH_FLOAT_TYPES(common_type, ctx, "div.out", CTYPE_IN, [&]() {
         ET_SWITCH_FLOAT_TYPES(out_type, ctx, "div.out", CTYPE_OUT, [&]() {
-          torch::executor::apply_binary_elementwise_fn<CTYPE_A, CTYPE_B, CTYPE_OUT>(
+          torch::executor::
+            apply_binary_elementwise_fn<CTYPE_A, CTYPE_B, CTYPE_OUT>(
               [](const CTYPE_A val_a, const CTYPE_B val_b) {
                 CTYPE_IN a_casted = static_cast<CTYPE_IN>(val_a);
                 CTYPE_IN b_casted = static_cast<CTYPE_IN>(val_b);
@@ -188,13 +187,13 @@ Tensor& div_out_mode(
   int max_dim = a.dim() > b.dim() ? a.dim() : b.dim();
   max_dim = out.dim() > max_dim ? out.dim() : max_dim;
   
-  if((a_type != ScalarType::Float) || (b_type != ScalarType::Float))
+  if ((a_type != ScalarType::Float) || (b_type != ScalarType::Float))
     optimized = 0;
   
-  if((a_dim == 0) || (b_dim == 0))
+  if ((a_dim == 0) || (b_dim == 0))
     optimized = 0;
 
-  if((broadcast == 1) && (max_dim > kNnlibMaxDim))
+  if ((broadcast == 1) && (max_dim > kNnlibMaxDim))
     optimized = 0;
   int mode_val = -1;
   if (mode.has_value() && mode.value() == "trunc") 
@@ -204,20 +203,17 @@ Tensor& div_out_mode(
   else
     optimized = 0;
       
-  if(optimized)
-  {
+  if (optimized) {
     float* a_data = a.mutable_data_ptr<float>();
     float* b_data = b.mutable_data_ptr<float>();
     float* out_data = out.mutable_data_ptr<float>();
 
-    if(broadcast)
-    {
+    if (broadcast) {
       int out_shape[kNnlibMaxDim];
       int inp1_shape[kNnlibMaxDim];
       int inp2_shape[kNnlibMaxDim];
       
-      for(int i = 0; i < kNnlibMaxDim; i++)
-      {
+      for (int i = 0; i < kNnlibMaxDim; i++) {
         inp1_shape[i] = 1;
         inp2_shape[i] = 1;
         out_shape[i] = 1;
@@ -227,18 +223,20 @@ Tensor& div_out_mode(
       int off_a = kNnlibMaxDim - a.dim();
       int off_b = kNnlibMaxDim - b.dim();
 
-      for(int i = 0; i < out.dim(); i++)
+      for (int i = 0; i < out.dim(); i++)
         out_shape[i+off_o] = out.size(i);
-      for(int i = 0; i < a.dim(); i++)
+      for (int i = 0; i < a.dim(); i++)
         inp1_shape[i+off_a] = a.size(i);
-      for(int i = 0; i < b.dim(); i++)
+      for (int i = 0; i < b.dim(); i++)
         inp2_shape[i+off_b] = b.size(i);
       
-      xa_nn_elm_div_mode_broadcast_4D_f32xf32_f32(out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape, mode_val);
+      xa_nn_elm_div_mode_broadcast_4D_f32xf32_f32(
+        out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape, mode_val);
     }
     else
     {
-      xa_nn_elm_div_mode_f32xf32_f32(out_data, a_data, b_data, out.numel(), mode_val);
+      xa_nn_elm_div_mode_f32xf32_f32(
+        out_data, a_data, b_data, out.numel(), mode_val);
     }
     
     return out;
@@ -248,7 +246,8 @@ Tensor& div_out_mode(
     ET_SWITCH_REAL_TYPES_AND(Bool, b_type, ctx, "div.out_mode", CTYPE_B, [&]() {
       ET_SWITCH_FLOAT_TYPES(common_type, ctx, "div.out_mode", CTYPE_IN, [&]() {
         ET_SWITCH_REAL_TYPES(out_type, ctx, "div.out_mode", CTYPE_OUT, [&]() {
-          torch::executor::apply_binary_elementwise_fn<CTYPE_A, CTYPE_B, CTYPE_OUT>(
+          torch::executor::
+            apply_binary_elementwise_fn<CTYPE_A, CTYPE_B, CTYPE_OUT>(
               [mode](const CTYPE_A val_a, const CTYPE_B val_b) {
                 CTYPE_IN a_casted = static_cast<CTYPE_IN>(val_a);
                 CTYPE_IN b_casted = static_cast<CTYPE_IN>(val_b);
@@ -272,6 +271,6 @@ Tensor& div_out_mode(
 }
 
 
-} // namespace impl
-} // namespace HiFi
 } // namespace native
+} // namespace HiFi
+} // namespace impl
diff --git a/backends/cadence/hifi/operators/op_mul.cpp b/backends/cadence/hifi/operators/op_mul.cpp
index 9200d980..1b2e62cd 100644
--- a/backends/cadence/hifi/operators/op_mul.cpp
+++ b/backends/cadence/hifi/operators/op_mul.cpp
@@ -6,12 +6,12 @@
  * LICENSE file in the root directory of this source tree.
  */
 
+#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 #include <executorch/kernels/portable/cpu/scalar_utils.h>
 #include <executorch/kernels/portable/cpu/util/broadcast_util.h>
 #include <executorch/kernels/portable/cpu/util/functional_util.h>
 #include <executorch/runtime/kernel/kernel_includes.h>
 #include <executorch/runtime/platform/assert.h>
-#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 
 using exec_aten::Scalar;
 using exec_aten::ScalarType;
@@ -86,7 +86,7 @@ mul_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
   ScalarType common_type = promoteTypes(a_type, b_type, /*half_to_float*/ true);
   ScalarType out_type = out.scalar_type();
   constexpr int kNnlibMaxDim = 4;  /*fallback if broadcast and dim > 4 */
-  
+
   int a_dim = a.dim(), b_dim = b.dim(), out_dim = out.dim();
   bool optimized = 1;
   /*find broadcast*/
@@ -97,28 +97,25 @@ mul_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
   max_dim = out.dim() > max_dim ? out.dim() : max_dim;
 
   
-  if((a_type != ScalarType::Float) || (b_type != ScalarType::Float))
+  if ((a_type != ScalarType::Float) || (b_type != ScalarType::Float))
     optimized = 0;
   
-  if( (a_dim == 0) || (b_dim == 0) )
+  if ((a_dim == 0) || (b_dim == 0) )
     optimized = 0;
   
-  if((broadcast == 1) && (max_dim > kNnlibMaxDim))
+  if ((broadcast == 1) && (max_dim > kNnlibMaxDim))
     optimized = 0;
 
-  if(optimized)
-  {
+  if (optimized) {
     float* a_data = a.mutable_data_ptr<float>();
     float* b_data = b.mutable_data_ptr<float>();
     float* out_data = out.mutable_data_ptr<float>();
 
-    if(broadcast == 1)
-    {
+    if (broadcast == 1) {
        int out_shape[kNnlibMaxDim];
        int inp1_shape[kNnlibMaxDim];
        int inp2_shape[kNnlibMaxDim];
-       for(int i = 0; i < kNnlibMaxDim; i++)
-       {
+       for (int i = 0; i < kNnlibMaxDim; i++) {
           out_shape[i] = 1;
           inp1_shape[i] = 1;
           inp2_shape[i] = 1;
@@ -126,14 +123,15 @@ mul_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
        int off_o = kNnlibMaxDim - out.dim();
        int off_a = kNnlibMaxDim - a.dim();
        int off_b = kNnlibMaxDim - b.dim();
-       for(int i = 0; i < out.dim(); i++){
-            out_shape[i+off_o] = out.size(i);}
-       for(int i = 0; i < a.dim(); i++)
+       for (int i = 0; i < out.dim(); i++)
+            out_shape[i+off_o] = out.size(i);
+       for (int i = 0; i < a.dim(); i++)
             inp1_shape[i+off_a] = a.size(i);
-       for(int i = 0; i < b.dim(); i++)
+       for (int i = 0; i < b.dim(); i++)
             inp2_shape[i+off_b] = b.size(i);
         
-       xa_nn_elm_mul_broadcast_4D_f32xf32_f32(out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape);
+       xa_nn_elm_mul_broadcast_4D_f32xf32_f32(
+        out_data, out_shape, a_data, inp1_shape, b_data, inp2_shape);
     }
     else
     {
@@ -154,7 +152,7 @@ mul_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
             CTYPE_A,
             CTYPE_B,
             CTYPE_IN,
-            CTYPE_OUT>::run(a, b, out); 
+            CTYPE_OUT>::run(a, b, out);
       });
     });
   });
@@ -162,6 +160,6 @@ mul_out(RuntimeContext& ctx, const Tensor& a, const Tensor& b, Tensor& out) {
   return out;
 }
 
-} // namespace impl
-} // namespace HiFi
 } // namespace native
+} // namespace HiFi
+} // namespace impl
diff --git a/backends/cadence/hifi/operators/op_sigmoid.cpp b/backends/cadence/hifi/operators/op_sigmoid.cpp
index fa408d4b..1ed89880 100644
--- a/backends/cadence/hifi/operators/op_sigmoid.cpp
+++ b/backends/cadence/hifi/operators/op_sigmoid.cpp
@@ -8,9 +8,9 @@
 
 #include <cmath>
 
+#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 #include <executorch/kernels/portable/cpu/util/functional_util.h>
 #include <executorch/runtime/kernel/kernel_includes.h>
-#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 
 using exec_aten::ScalarType;
 using exec_aten::Tensor;
@@ -18,7 +18,7 @@ using executorch::aten::RuntimeContext;
 using torch::executor::Error;
 
 namespace impl {
-namespace HiFi { 
+namespace HiFi {
 namespace native {
 
 using Tensor = exec_aten::Tensor;
@@ -40,13 +40,12 @@ Tensor& sigmoid_out(RuntimeContext& ctx, const Tensor& in, Tensor& out) {
 
   ScalarType in_type = in.scalar_type();
   ScalarType out_type = out.scalar_type();
-  
+
   bool optimized = 1;
-  if((in_type != ScalarType::Float) || (out_type != ScalarType::Float))
+  if ((in_type != ScalarType::Float) || (out_type != ScalarType::Float))
       optimized = 0;
   
-  if(optimized)
-  {
+  if (optimized) {
     float* data_in = in.mutable_data_ptr<float>();
     float* data_out = out.mutable_data_ptr<float>();
     xa_nn_vec_sigmoid_f32_f32(data_out, data_in, in.numel());
diff --git a/backends/cadence/hifi/operators/op_sub.cpp b/backends/cadence/hifi/operators/op_sub.cpp
index b9f35caf..d9958bf8 100644
--- a/backends/cadence/hifi/operators/op_sub.cpp
+++ b/backends/cadence/hifi/operators/op_sub.cpp
@@ -6,25 +6,25 @@
  * LICENSE file in the root directory of this source tree.
  */
 
+#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 #include <executorch/kernels/portable/cpu/scalar_utils.h>
 #include <executorch/kernels/portable/cpu/util/broadcast_util.h>
 #include <executorch/kernels/portable/cpu/util/functional_util.h>
 #include <executorch/kernels/portable/cpu/util/kernel_ops_util.h>
 #include <executorch/runtime/kernel/kernel_includes.h>
 #include <executorch/runtime/platform/assert.h>
-#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 
 using exec_aten::Scalar;
 using exec_aten::ScalarType;
 using exec_aten::Tensor;
+using executorch::aten::RuntimeContext;
 using executorch::runtime::can_cast;
 using executorch::runtime::CppTypeToScalarType;
-using executorch::aten::RuntimeContext;
 using torch::executor::Error;
 
 
 namespace impl {
-namespace HiFi { 
+namespace HiFi {
 namespace native {
 
 namespace {
@@ -92,7 +92,8 @@ Tensor& sub_out(
 
   ScalarType a_type = a.scalar_type();
   ScalarType b_type = b.scalar_type();
-  ScalarType alpha_type = torch::executor::native::utils::get_scalar_dtype(alpha);
+  ScalarType alpha_type =
+    torch::executor::native::utils::get_scalar_dtype(alpha);
   ScalarType common_type = promoteTypes(a_type, b_type, /*half_to_float*/ true);
   ScalarType out_type = out.scalar_type();
 
@@ -115,18 +116,17 @@ Tensor& sub_out(
   int max_dim = a.dim() > b.dim() ? a.dim() : b.dim();
   max_dim = out.dim() > max_dim ? out.dim() : max_dim;
   
-  if((out_type != ScalarType::Float) || (alpha_val != 1.0))
+  if ((out_type != ScalarType::Float) || (alpha_val != 1.0))
     optimized = 0;
   
-  if((a_dim == 0) || (b_dim == 0))
+  if ((a_dim == 0) || (b_dim == 0))
     optimized = 0;
 
-  if((broadcast == 1) && (max_dim > kNnlibMaxDim))
+  if ((broadcast == 1) && (max_dim > kNnlibMaxDim))
     optimized = 0;
   
 
-  if(optimized)
-  {
+  if (optimized) {
       /*logic to find broadcast*/
       const int a_is_broadcasted = !out.sizes().equals(a.sizes());
       const int b_is_broadcasted = !out.sizes().equals(b.sizes());
@@ -135,14 +135,12 @@ Tensor& sub_out(
       const float* const a_data = a.const_data_ptr<float>();
       const float* const b_data = b.const_data_ptr<float>();
       float* const out_data = out.mutable_data_ptr<float>();
-      if(broadcast == 1)
-      {
+      if (broadcast == 1) {
          int out_shape[kNnlibMaxDim];
          int inp1_shape[kNnlibMaxDim];
          int inp2_shape[kNnlibMaxDim];
          
-         for(int i = 0; i < kNnlibMaxDim; i++)
-         {
+         for (int i = 0; i < kNnlibMaxDim; i++) {
             out_shape[i] = 1;
             inp1_shape[i] = 1;
             inp2_shape[i] = 1;
@@ -151,14 +149,15 @@ Tensor& sub_out(
          int off_o = kNnlibMaxDim - out_dim;
          int off_a = kNnlibMaxDim - a_dim;
          int off_b = kNnlibMaxDim - b_dim;
-         for(int i = 0; i < out_dim; i++)
+         for (int i = 0; i < out_dim; i++)
              out_shape[i+off_o] = out.size(i);
-         for(int i = 0; i < a_dim; i++)
+         for (int i = 0; i < a_dim; i++)
              inp1_shape[i+off_a] = a.size(i);
-         for(int i = 0; i < b_dim; i++)
+         for (int i = 0; i < b_dim; i++)
              inp2_shape[i+off_b] = b.size(i);
 
-         xa_nn_elm_sub_broadcast_4D_f32xf32_f32(out_data, out_shape, a_data, inp1_shape,b_data, inp2_shape);
+         xa_nn_elm_sub_broadcast_4D_f32xf32_f32(
+           out_data, out_shape, a_data, inp1_shape,b_data, inp2_shape);
       }                      
       else
       {
@@ -190,6 +189,6 @@ Tensor& sub_out(
   return out;
 }
 
-} // namespace impl
-} // namespace HiFi
 } // namespace native
+} // namespace HiFi
+} // namespace impl
diff --git a/backends/cadence/hifi/operators/op_tanh.cpp b/backends/cadence/hifi/operators/op_tanh.cpp
index a80450b8..7989ac3b 100644
--- a/backends/cadence/hifi/operators/op_tanh.cpp
+++ b/backends/cadence/hifi/operators/op_tanh.cpp
@@ -6,10 +6,10 @@
  * LICENSE file in the root directory of this source tree.
  */
 
+#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 #include <executorch/kernels/portable/cpu/pattern/pattern.h>
 #include <executorch/runtime/kernel/kernel_includes.h>
 #include <cmath>
-#include <executorch/backends/cadence/hifi/kernels/kernels.h>
 
 using exec_aten::ScalarType;
 using exec_aten::Tensor;
@@ -17,28 +17,29 @@ using executorch::aten::RuntimeContext;
 using torch::executor::Error;
 
 namespace impl {
-namespace HiFi { 
+namespace HiFi {
 namespace native {
 
 
 Tensor& tanh_out(RuntimeContext& ctx, const Tensor& in, Tensor& out) {
 
   bool optimized = 1;
-  if((in.scalar_type() != ScalarType::Float) || (out.scalar_type() != ScalarType::Float))
-      optimized = 0;
+  if ((in.scalar_type() != ScalarType::Float) || 
+      (out.scalar_type() != ScalarType::Float))
+    optimized = 0;
   
-  if(optimized)
-  {
+  if (optimized) {
     float* data_in = in.mutable_data_ptr<float>();
     float* data_out = out.mutable_data_ptr<float>();
     xa_nn_vec_tanh_f32_f32(data_out, data_in, (int)in.numel());
     return out;
   }
 
-  return torch::executor::native::internal::unary_ufunc_realhb_to_floath(std::tanh, ctx, in, out);
+  return torch::executor::native::internal::unary_ufunc_realhb_to_floath(
+    std::tanh, ctx, in, out);
 
 }
 
-} // namespace impl
-} // namespace HiFi
 } // namespace native
+} // namespace HiFi
+} // namespace impl
